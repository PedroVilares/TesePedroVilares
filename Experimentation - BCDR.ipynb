{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mammography - Breast Cancer\n",
    "\n",
    "## Prediction of Breast Cancer\n",
    "\n",
    "\n",
    "\n",
    "#### University of Aveiro\n",
    "\n",
    "### Contents\n",
    "* [1. Packages](#id1)\n",
    "* [2. Helper Functions](#id2)\n",
    "* [3. Reading .csv Data](#id3)\n",
    "    - [3.1. Training set](#id3.1)\n",
    "    - [3.2. Validation set](#id3.2)\n",
    "    - [3.3. Overview the Data Set](#id3.3)\n",
    "    - [3.4. Image Visualization](#id3.4)\n",
    "    - [3.5. Selecting Data](#id3.5)\n",
    "* [4. Classification Models](#id4)\n",
    "    - [4.1. Kooi et al. Network architecture](#id4.1)\n",
    "    - [4.2. Transfer Learning - VGG16](#id4.2)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "### References\n",
    "* CBIS_DDSM: https://wiki.cancerimagingarchive.net/display/Public/CBIS-DDSM\n",
    "* NumPy Lib: https://numpy.org/\n",
    "* Pandas Lib: https://pandas.pydata.org/\n",
    "* Scikit-Learn Lib: https://scikit-learn.org/stable/\n",
    "* TensorFlow: https://www.tensorflow.org/\n",
    "* Keras: https://keras.io/\n",
    "* Pydicom: https://pydicom.github.io/\n",
    "* Unbalanced Samples: https://medium.com/strands-tech-corner/unbalanced-datasets-what-to-do-144e0552d9cd\n",
    "* Oversampling: https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18\n",
    "* Large scale deep learning for computer aided detection of mammographic lesions”, Medical Image Analysis, 2016. http://dx.doi.org/10.1016/j.media.2016.07.007\n",
    "* Keras CNN Dog or Cat Classification: https://www.kaggle.com/uysimty/keras-cnn-dog-or-cat-classification\n",
    "* Breast cancer classification with Keras and Deep Learning: https://www.pyimagesearch.com/2019/02/18/breast-cancer-classification-with-keras-and-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Packages <a class=\"anchor\" id=\"id1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imageio as io\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#tensorflow related\n",
    "import tensorflow as tf\n",
    "print(\"TF version:\", tf.__version__)\n",
    "import pydicom as dicom\n",
    "from tensorflow import keras\n",
    "from keras import callbacks\n",
    "\n",
    "#.py files\n",
    "import generate_data as generator\n",
    "import preprocessing as prepr\n",
    "import models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Helper Functions <a class=\"anchor\" id=\"id2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_classifications(features_dataframe,images_dataframe):\n",
    "    \"\"\"\n",
    "    Remove space from patient name\n",
    "    \"\"\"\n",
    "\n",
    "    features_patient_id= features_dataframe['patient_id']\n",
    "    features_classification= features_dataframe['classification']\n",
    "    patient_id_classification_dict=dict()\n",
    "    i=0\n",
    "    for patient_id in features_patient_id:\n",
    "        if patient_id in list(patient_id_classification_dict.keys()):\n",
    "            i+=1\n",
    "        else:\n",
    "            patient_id_classification_dict[patient_id] = features_classification[i]\n",
    "            i+=1\n",
    "    \n",
    "    images_classification = []\n",
    "    images_patient_id= images_dataframe['patient_id']\n",
    "    for patient_id in images_patient_id:\n",
    "        images_classification.append(patient_id_classification_dict[patient_id])\n",
    "\n",
    "    return images_classification\n",
    "\n",
    "def fix_path(path_list):\n",
    "\n",
    "    new_paths = []\n",
    "    n=0\n",
    "    for path in path_list:\n",
    "        lost_path = path.split('/')\n",
    "        if lost_path[1][0] == ' ':\n",
    "            lost_path[1]= lost_path[1][1:]\n",
    "        string_path= lost_path[0]\n",
    "        for i in lost_path[1:len(lost_path)]:\n",
    "            string_path = string_path+'/'+i\n",
    "        new_paths.append(string_path)\n",
    "        n += 1\n",
    "        \n",
    "    return new_paths\n",
    "\n",
    "def dataframe_by_view(dataframe,view):\n",
    "    \"\"\"\n",
    "    Builds dataframe with images taken from view.\n",
    "    View must be 'CC' or 'O\n",
    "    \"\"\"\n",
    "    image_paths = dataframe['paths']\n",
    "    image_labels = dataframe['labels']\n",
    "    view_paths = []\n",
    "    view_labels = []\n",
    "    i=0\n",
    "    for path in image_paths:\n",
    "        if view in path:\n",
    "            view_paths.append(path)\n",
    "            view_labels.append(image_labels[i])\n",
    "        i+=1\n",
    "    view_dataframe = pd.DataFrame({'paths':view_paths,'labels':view_labels})    \n",
    "\n",
    "    return view_dataframe\n",
    "\n",
    "#evaluate predicted data\n",
    "#confusion matrix and classification report\n",
    "def evaluation(model,data_test,data_predicted):\n",
    "    classes= ['Benign','Malign']\n",
    "    report= classification_report(data_test,data_predicted,[0.0,1.0],classes)\n",
    "    classes1 = ['Benign\\n(Predicted)','Malign\\n(Predicted)']\n",
    "    classes2 = ['Benign\\n(Real)','Malign\\n(Real)']\n",
    "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n",
    "    c_matrix = confusion_matrix(data_test, data_predicted)\n",
    "    #c_matrix = c_matrix.astype('float') / c_matrix.sum(axis = 1)[:, np.newaxis]\n",
    "    c_train = pd.DataFrame(c_matrix, index = classes2, columns = classes1)\n",
    "    \n",
    "    plt.figure(figsize = (16,4))\n",
    "    ax = sns.heatmap(c_train, annot = True, cmap = cmap, square = True, cbar = False,fmt = '.2f', annot_kws = {\"size\": 20})\n",
    "    print(\"Report: \\n\",report) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reading .csv Data <a class=\"anchor\" id=\"id3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. BCDR Dataset number 1<a class=\"anchor\" id=\"id3.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcdr1 = pd.read_csv('BCDR-D01_dataset/bcdr_d01_img.csv')\n",
    "bcdr1_labels = pd.read_csv('BCDR-D01_dataset/bcdr_d01_features.csv')\n",
    "print('Suspicious Dataset Nº 1: ',bcdr1.shape)\n",
    "#bcdr1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. BCDR Dataset 2 <a class=\"anchor\" id=\"id3.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcdr2 = pd.read_csv('BCDR-D02_dataset/bcdr_d02_img.csv')\n",
    "bcdr2_labels = pd.read_csv('BCDR-D02_dataset/bcdr_d02_features.csv')\n",
    "print('Suspicious Dataset Nº 2: ',bcdr2.shape)\n",
    "#bcdr2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. BCDR Dataset 3 <a class=\"anchor\" id=\"id3.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcdrN = pd.read_csv('BCDR-DN01_dataset/bcdr_dn01_img.csv')\n",
    "print('Normal Dataset: ',bcdrN.shape)\n",
    "#bcdrN.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert TIF to TIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_broken_paths= []\n",
    "for path in bcdrN['image_filename']:\n",
    "    normal_broken_paths.append('BCDR-DN01_dataset/'+path)\n",
    "normal_paths_tif = fix_path(normal_broken_paths)\n",
    "\n",
    "normal_paths= generator.convert(normal_paths_tif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious_broken_paths_1= []\n",
    "for path in bcdr1['image_filename']:\n",
    "    suspicious_broken_paths_1.append('BCDR-D01_dataset/'+path)\n",
    "suspicious_paths_1_tif = fix_path(suspicious_broken_paths_1)\n",
    "\n",
    "suspicious_paths_1 = generator.convert(suspicious_paths_1_tif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#suspicious_broken_paths_2= []\n",
    "#for path in bcdr2['image_filename']:\n",
    "#    suspicious_broken_paths_2.append('BCDR-D02_dataset/'+path)\n",
    "#suspicious_paths_2_tif = fix_path(suspicious_broken_paths_2)\n",
    "\n",
    "#suspicious_paths_2 = generator.convert(suspicious_paths_2_tif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Normal vs Suspicious Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcdr_normal = pd.DataFrame({'paths':normal_paths,'labels':'Normal'})\n",
    "\n",
    "bcdr_normal_cc = dataframe_by_view(dataframe=bcdr_normal,view='CC')\n",
    "bcdr_normal_mlo = dataframe_by_view(bcdr_normal,'O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#suspicious_dfs = [pd.DataFrame({'paths':suspicious_paths_1,'labels':'Suspicious'}),pd.DataFrame({'paths':suspicious_paths_2,'labels':'Suspicious'})]\n",
    "\n",
    "#bcdr_suspicious = pd.concat(suspicious_dfs,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcdr_suspicious = pd.DataFrame({'paths':suspicious_paths_1,'labels':'Suspicious'})\n",
    "\n",
    "bcdr_suspicious_cc = dataframe_by_view(bcdr_suspicious,'CC')\n",
    "bcdr_suspicious_mlo = dataframe_by_view(bcdr_suspicious,'O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking dataset balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Normal Images: ',bcdr_normal.shape[0])\n",
    "print('Normal CC: ',bcdr_normal_cc.shape[0])\n",
    "print('Normal MLO: ',bcdr_normal_mlo.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Suspicious Images: ',bcdr_suspicious.shape[0])\n",
    "print('Suspicious CC: ',bcdr_suspicious_cc.shape[0])\n",
    "print('Suspicious MLO: ',bcdr_suspicious_mlo.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Images: ',bcdr_normal.shape[0]+bcdr_suspicious.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Training vs Validation Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split=0.2\n",
    "validation_images = round(0.2*(bcdr_normal.shape[0]+bcdr_suspicious.shape[0]))\n",
    "print('Validation Images: ',round(validation_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df_cc = pd.concat([\n",
    "    bcdr_suspicious_cc[:round((validation_images/4))],\n",
    "    bcdr_normal_cc[:round((validation_images/4))]],\n",
    "    ignore_index=True\n",
    "    )\n",
    "\n",
    "validation_df_mlo = pd.concat([\n",
    "    bcdr_suspicious_mlo[:round((validation_images/4))],\n",
    "    bcdr_normal_mlo[:round((validation_images/4))]],\n",
    "    ignore_index=True    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df_cc = pd.concat([\n",
    "    bcdr_suspicious_cc[round(validation_images/4):],\n",
    "    bcdr_normal_cc[round(validation_images/4):]],ignore_index=True)\n",
    "\n",
    "training_df_mlo = pd.concat([\n",
    "    bcdr_suspicious_mlo[round(validation_images/4):],\n",
    "    bcdr_normal_mlo[round(validation_images/4):]],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size_1 = 2500\n",
    "target_size_2 = 2000\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator_cc = generator.generate(training_df_cc,target_size_1,target_size_2,batch_size)\n",
    "training_generator_mlo = generator.generate(training_df_mlo,target_size_1,target_size_2,batch_size)\n",
    "validation_generator_cc = generator.generate(validation_df_cc,target_size_1,target_size_2,batch_size)\n",
    "validation_generator_mlo = generator.generate(validation_df_mlo,target_size_1,target_size_2,batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIFF Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_image =plt.imread(normal_paths[0])\n",
    "plt.imshow(np.asarray(tiff_image))\n",
    "print(tiff_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_image= training_generator[0][0][0]\n",
    "plt.imshow(np.asarray(generated_image)[:,:,0],cmap='gray')\n",
    "print(generated_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.create_sequential_model(target_size_1,target_size_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer='adam'\n",
    "loss_function='categorical_crossentropy'\n",
    "metrics=[keras.metrics.categorical_accuracy,keras.metrics.AUC]\n",
    "epochs= 30\n",
    "steps_per_epoch= 20\n",
    "fit_callbacks = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor= 'val_accuracy',\n",
    "        min_delta= 0.01,\n",
    "        patience= 8, \n",
    "        restore_best_weights= True),\n",
    "    callbacks.CSVLogger(\n",
    "        filename= 'BCDR Training Parameters',\n",
    "        separator=\",\", \n",
    "        append=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model,to_file='sequential_model.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = models.train_model(model,optimizer,loss_function,metrics,epochs,steps_per_epoch,training_generator_cc,fit_callbacks)"
   ]
  },
  {
   "source": [
    "## Classification with Sequential Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.create_functional_model(target_size_1,target_size_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator_cc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=keras.optimizers.Adam(learning_rate=0.00001)\n",
    "loss_function='categorical_crossentropy'\n",
    "metrics=[keras.metrics.categorical_accuracy,keras.metrics.AUC]\n",
    "epochs= 30\n",
    "steps_per_epoch= 20\n",
    "fit_callbacks = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor= 'val_accuracy',\n",
    "        min_delta= 0.01,\n",
    "        patience= 8, \n",
    "        restore_best_weights= True),\n",
    "    callbacks.CSVLogger(\n",
    "        filename= 'BCDR Training Parameters',\n",
    "        separator=\",\", \n",
    "        append=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model,show_shapes=True,to_file='functional_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,loss=loss_function,metrics='AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x={'CC_Input':training_generator_cc[0][0][0],'MLO_Input':training_generator_mlo[0][0][0]},y={'CC_outputtraining_generator_cc[0][1][0],training_generator_mlo[0][1][0]},epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator_cc[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(trained_model.history['loss'])\n",
    "plt.plot(trained_model.history['val_loss'])\n",
    "plt.legend(['Training', 'Validation'])\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Categorical Accuracy')\n",
    "plt.plot(trained_model.history['categorical_accuracy'])\n",
    "plt.plot(trained_model.history['val_categorical_accuracy'])\n",
    "plt.legend(['Training', 'Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1\n",
    "for predictions in y_predicted:\n",
    "    if predictions[0] > 0.5:\n",
    "        prediction = 'Normal'\n",
    "        confidence = float(\"{:.2f}\".format(predictions[0]))*100\n",
    "    else:\n",
    "        prediction = 'Suspicious'\n",
    "        confidence = float(\"{:.2f}\".format(predictions[1]))*100\n",
    "    print('Image {} predicted as {} with {} confidence'.format(n,prediction,confidence))\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "66c06bc2ee02d0626a5e4113814e582df1fd7a3f49750ed90ecf0f9636802fa3"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}